IAAS --> Virtual network / virtual machines

PAAS --> Cloud services (Playstore)

SAAS --> Applications hosted on cloud (Google mail, CRM, ERP)

IDAAS --> SOS, Multi factor authentication (Az active directory/entra id, AWS IAM)

NAAS --> AWS direct connect, AZ networking

Deployment models
=================
Public --> Shared by multiple organisations
Private --> Private to a organisation/individual
Hybrid --> Combination of private and public
Multi-cloud--> Combination of services from different providers
Community --> Shared across a community

Az services
=========
Compute services
===============
Virtual machines
Service fabric
Batch
Scheduler

Networking services
=================
Virtual networks
Traffic manager
Express route

Identity and access management
===========================
Active directory/ms entra connect
Key vault
Multi-factor authentication

Media and CDN
=============
CDN
Media services

Web and mobile services
=====================
Function apps
Web apps
API apps & management

Analytics
========
HD insights
ML
Streaming --> IOt hub, Event hub
Data factory
Synapse

Storage and back up
=================
Az blob storage
Datlake gen 1
Files
back up
site recovery

database
========
sql
Cosmos
redis

Integration services
================
Service bus
Biztalk
Storage queues
Event grid
Logic apps

Management
===========
Key-vault
Portal
Automation

Commerce
=========
VM deployment
Store/market place

Developer services
================
Visual studio
App insights

Serverless computing
==================
Azure functions
Logic apps
Event grid

ML
===
Az ML studio

Virtual machines
==============
VM is a virtual computer with hardware/software etc.

Isolation
Internet communication
Traffic filtering
On-premise connectivity

OSI layers
========

Application layer --> Human - computer interaction layer where applications can access the network service

Presentation layer --> Ensures that the data is in a usable format and is where the data encryption happens

Session layer --> Maintains the connections and is responsible for controlling ports and sessions

Transport layer --> Transmits data using transaction protocols including TCP and UDP

Network layer --> Decides which physical path the data will follow

Data link layer --> Defines the format of data on the network

Physical layer --> Transmits the raw bit of data streams over the physical medium

Socket --> IP + Port
HTTP--> 80
SSH --> 22
HTTPS --> 443

Azure bastion --> Used to SSH/connect to your VM if you don't have a public IP assigned to the VM

Virtual Network
=============
Act as a communication channel between resources launched in the cloud.

Azure Vnet
=========
Representation of your/organisation's own network in the cloud, logical isolation of azure cloud dedicated to your subscription

Subnet --> Subnetwork
Route table
Network Security Groups --> Firewalls, responsible for filtering incoming, outgoingtraffic from your VM's
Network Interface Card

IP addresses   --> Internet protocol --> Unique numerical address
==========
Public --> Globally unique, accessible from the internet
Private --> Local network, not accessible from the internet
Static IP --> Manually assigned, remains constant
Dynamic IP --> Automatically assigned, changes periodically

IPV4 (32 bit)
IPV6 (128 bit)

_,_,_,_

0-255,0-255,0-255,0-255

0.0.0.0 to 255.255.255.255

192.168.0.0 to 192.168.0.255 = 256
192.168.0.1
192.168.0.2
192.168.0.3
.
.
.
.
.
192.168.0.255

CIDR --> Classless Inter Domain Routing
================================
192.168.170.0  - 192.168.170.255 --> 256 unique addresses --> 192.168.170.0/24
192.172.0.0  - 192.172.255.255 --> 192.172.0.0/16 --> 256*256
192.0.0.0/8 --> 256*256*256

Preferred IP series for intranet (private IP)

small network --> 192.168.1.x --> 2**8
Large network --> 172.16.x.x --> 2**16


Security
========
Network Security Groups --> Firewalls, responsible for filtering incoming, outgoing traffic from your VM's
Application security groups --> Provide for the grouping of servers with similar ports filtering requirements, group together servers with similar functions like webservers

Azure Active directory --> entra id
===================================
Authentication
Single sign On [sso]
Application management
b2b
b2c
Device management
multi-factor authentication
ddos


1. Identity management --> user group and device management
2. single sign on --> Access multiple applications with one set of credentials
3. Multi-factor authentication --> Enhanced security with additional verification steps
4. Directory integrations/conditional access

1. Key-vault service --> Secret management, key management, certificate management, storing secretes with a hardware security model (HSM's) 


Azure Advanced Threat protection (ATP)
Azure information protection

Governance
==========
Az policy
Policy inititives --> Definitions, Assignments
RBAC
Resource locks --> cannot delete, read only
Blueprints

Tags
====
Key-value pair

Azure monitor
=============
Collect, analyse and act on telemetry data from cloud and on-premise to improve performance

Azure health centre
===================
Analyse, respond, visualise and integrate

Subscription
=============
Dev
Testing
Production

Cost management
===============
Resource types
Services
Location

Tools
=====
Calculator
Advisor
Hub
Cost analysis
cost alerts

Azure storage
=============
PAAS
Managed cloud storage service --> high availability, durable, scalable and redundant storage

Files, messages, tables and other types of information

1 subscription --> 100 accounts --> 1 account * 500 tb

3 categories
============

1. Storage for virtual machines (like disks and files)
2. Unstructured data (videos, audio, images, messages, text) --> blobs & datalake stores
3. Structured data --> tables, azure sql db, cosmos db

types of storage
================
Blob storage --> Object storage --> Huge amount of unstructured data such as text, image, videos, audio, data for back up, disaster recovery or binary data
Files --> Highly available network file shares, uses SMB protocol (standard server message block) --> suitable for migration of data
Tables --> Structured sql/nosql data, key & value pair --> schemaless design
Queues --> Store and retrieve messages, each message of maximum size of 64 kb --> can store millions of messages

Performance
===========
Standard --> Backed by HDD (magnetic drives), lowest cost per gb, best for applications that requires bulk storage, data is accesses infrequently
Premium --> Are backed SSD (Solid state drives), offers low latency and consistant performance, use with axure vm's disks, best for i/o intensive applications --> db


General purpose v1 --> Legacy, doesn't support performance tiers, 
General purpose v2
Blob storage


Performance tiers

Hot   --> Optimized for storing data that is accesses frequently like VM, files, images, videos that are in regular use --> storage cost (highest) and read cost (low)
Cool --> Optimied for storing data that is infrequently accessed and stored for atleast 30 days like back up data, storage cost(low) and read cost (higher than hot tier)
Archive --> Optimized for storing data that is really accessed and stored for atleast 180 days with flexible latency requirements, storage cost (least) and read cost (highest)


Blob containers & categories
============================
Containers --> Directory, can store many blobs under one container

Block blob --> Any text, video, audio, image files
Page blob --> random access files upto 8 tb --> Virtual hard drive --> serve as a disk for vm
Append blob --> Made up of multiple block blobs, optimized for append operations --> ideal for log data in vm

Storage replication
===================
LRS --> 3 copies of data in 3 storage boxes within the same data center
ZRS --> 3 copies of data in multiple data centres within same region
GRS --> 3 copies of data in primary region, 3 copies in a secondary region
RA-GRS --> Same as GRS, Read only access to secondary data centre
GZRS --> ZRS synchronously and single physical location in secondary region
RA-GRS --> same GZRS but read only access to the secondary region


Data lake gen2 combines the best features of the below 2,

Data lake
=========
Hadoop compatible
Optimized drivers

Blob storage
============
Low cost, storage tiers, high availability

                               Blob storage            ADL gen2


Access tiers                      yes                     yes

Top level Organization         Containers             Containers

Low level organisation         Virtual directory      Directory

Data containers                   Blob                  File

Soft delete                       yes                    no
    
Static website                    yes                    no

RBAC                              yes                    no

Access control list		   No                    yes

SAS                               yes                    yes

Access keys                       yes                    yes
 

Azure sql
=========
PAAS
Cloud based managed database services

Benefits
========
Convenient
Cost
Scale
Security
administration
SLA --> 99.99% uptime

MS
==
Hardware, data centre, virtualization, patching, OS, networking, security, auditing, scaling, back up/restore

DBA
===
Design, Development, Migrations, capacity planning, performance tuning, automation, optimization


Deployment models
=================

Single db --> Isolated & portable, contained, own set of resources

Elastic Pool --> Collection of single db's, shared set of resources, cost effective solution of the usage is unpredictable

Managed instances --> 100% compatibility with latest sql server, lift n shift/migration  


The vCore-based purchasing model lets you choose the number of vCores, the amount of memory, and the amount and speed of storage. 
The vCore-based purchasing model also allows you to use Azure Hybrid Benefit for SQL Server to gain cost savings by leveraging your existing SQL Server licenses.


The DTU-based purchasing model offers a blend of compute, memory, and I/O resources in three service tiers, to support light to heavy database workloads. 
Compute sizes within each tier provide a different mix of these resources, to which you can add additional storage resources.


Cosmos db
=========
Nosql db --> Columnar (Hbase, Cassandra), Document db (Mongo db, Cosmos db), Graph db(Neo4j, tiger graph), Key-value pair (Redi's, dynamo db)


APi's --> Core sql(rdbms), mongo(document), cassandra(columnar), gremlin(graph), table(key/value pair), postgre sql
api --> dot net, node js, python, java


{
"name":"arun",
"sal":8000,
"city":"blr"

Azure Synapse
=============
Enterprise data warehousing and big data analytics tool

SQL (synapse sql)
Spark (processing)
Pipelines (Integration) --> ETL, ELT (Az Data Factory) 

Integration with Cosmos db, ML, Power BI

Synapse workspace
=================
To build cloud based enterprise analytical solutions
Attached with ADLS gen 2 file system

SQL --> SQL pool --> run t-sql script on data in linked services

Serverless --> built-in --> Use sql without reserving any capacity.
Billing is based on the amount of data processed to run the query

Openrow set --> Allows you to access files in Az storage and return the content as a set of rows

Linked Services --> Connection strings, it defines the connection information required for workspace to connect with external resources

Synapse sql
===========
Ability to do T-sql based analytics in synapse workspace
2 consumption models,

Dedicated sql pool --> Consumes billable resources if it's active, pause the pool to reduce the costs and this is associated with a sql db
Serverless sql pool --> It comes by default without reserving any capacity, you're billed based on the amount of data processed to run a query

Data explorer pools --> IT can be used to run near real-time analytics on large volume of logs and time series data , streaming from applications, websites, IOT devices and more.

Apache spark for Synapse
========================
Ability to do spark based analytics in synapse workspace
Spark session is created when you choose spark pool

pool is like a runtime, it controls the resources used in the session

Spark notebook
Spark job definitions

Pipelines
=========

To move data between srvices & orchestrate activities.
Logical grouping of activities that performs a task together
Activities defines actions within pipelines --> Copying data, running a notebook or a sql/spark program

Triggers
Integration dataset

Azure Databricks
================
Spark based Big data analytics platform optimized for Azure

Data plane
Control plane

Workspace
=========
Environment to access all your databricks asssets

Notebooks
Libraries
Experimements

Cluster --> Set of computational resources and configurations on which you run data engineering, data science workloads, such as production ETL pipelines, streaming analytics,
ad-hoc analysis & ML model

Billing is made on a term called databricks unit (dbu)

Job clusters cannot be restarted

Folders
=======
All static assets, notebooks, experiments

Special folders
===============
Shared
users

Databricks data science & engineering platform--> Analytics
Databricks ML --> ML

Data components
===============

1. Tables --> Tables created using spark sql / databricks sql (stored in hive metastore), query it using sql
2. Data --> View all data cources, tables, files(dbfs) etc. 
3. DBFS --> Distributed file system, upload/download files/ store data for spark jobs
4. Hive catalogue --> Metadata repository for spark tables, manages table definitions, schemas, locations (compatible with hive ql)
5. Delta tables --> A opensource storage format for data lakes, ACID transactions, versioning, schema enforcement

6. Views
7. Datasets
8. External data sources

/FileStore/tables/b1.csv


Azure data factory
==================
Cloud based ETL for scheduling and managing workflows, orchestration and transformation of data at scale
Integration & data transformation

Linked services --> Connection strings, defines information needed for the data factory to connect with external resources

Datasets --> Represents the data structures within the data stores, which simply points to reference the data you want in your activities

Pipeline --> Logical grouping of activities that performs a unit of work

Activity --> Processing step in a pipeline

Data movement
Data transformation
Controlflow

Triggers --> Determines when a pipeline execution needs to be kicked off. 

Schedule trigger --> A trigger that involves a pipeline on a clock schedule
Tumbling window
Events


T-sql --> Transact sql --> Microsoft --> Azure sql database --> procedural programming
U-sql --> Big data analytics --> Az data lake analytics, az synapse --> scalable data processing

Dataflow --> Allows the developer to perform graphical data transformation

Azure Functions
===============

Install node.js
npm install -g azure-functions-core-tools
Install python <12.0 version
Set-ExecutionPolicy -ExecutionPolicy RemoteSigned -Scope CurrentUSer


Stream analytics Job
====================
2 type of inputs

Data stream --> IOT hub, Event Hub, Blob storage
Reference data --> SQL server, Blob storage, Files 

Event hub --> Application data
IOT hub --> IOT devices / sensors
Service hub --> Message broker which enables publish / subscribe 

1. Create azure IOT hub
2. Add a device to IOT hub
3. Pass the connection string to IOT web simulator
4. Create stream analytics job
5. Create Power BI dashboard / Blob storage

IOT hub

Event hub
=========
ML studio
==========
Designer / Pipeline

Auto ML --> datasets --> register data

build a new auto ml project

{
  "input_data": {
    "columns": [
      "CreditScore",
      "Age",
      "Tenure",
      "Balance",
      "NumOfProducts",
      "HasCrCard",
      "IsActiveMember",
      "EstimatedSalary"
    ],
    "index": [0],
    "data": [[619,42,2,0,1,1,1,101348.88]
]
  }
}


Python SDK

Data lake storage gen 1 --> Optimized for Hadoop and Spark workloads, supports hive, pig and mapreduce

Data lake storage gen 2 --> Optimised for blob storage, analytics, ai and ml, high level security, supports ADF, synapse, databricks etc.

Azure Data lake Analytics --> Dstributed analytics service built on top of YARN
Microsoft Azure doesn't provide data lake analytics service anymore.


1. Build and deploy a ML model on the given data using Designer, Auto ML and Python SDK
MTcars

2. Upload the data into Azure blob storage, access it in Databricks, analyse and prepare reports

Software Development Life Cycle
================================

1. Waterfall --> Sequential, linear

Scope of change is little
Good for simple, cost affective well defined, limited time period projects


2. Agile

Iterations, project is delivered in iterations/sprints
Scope of change is likely
Allows to collect the feedback often from the clients, continuous improvement

Methodologies
=============
Scrum
Kanban
Lean

Benefits
========
Improved customer satisfaction, reduced project risk

Challenges
==========
Cultural shift, Scaling to larger teams, lack of clear requirements


3. Devops

Development + Operations

Culture --> Collaboration between Development & Operations team, they work as 1 team over here.

Phases --> CICD pipelines
=========================

Continous Development --> Version control --> Git, Bit bucket,sub version 

Continous Testing --> Selenium/ selenium grid --> Automation testing software, Opensource web driver 

Continous Integration --> Jenkins --> Opensource, 1000+ plugins

Continous Deployment --> Puppet, Chef, Ansible, Saltstack

Continous monitoring --> Splunk, Nagios, Grafana


Build tools --> Maven, Gradle, Ant

Tools
=====
Dockers --> Containers allows you to package your applications & its dependencies
Dockerhub --> Cloud based containarisation platform
Kubernetes --> Container management technology.

Puppet --> Operational tool, Define configurations for each & every host, scaaling up/down is dynamic

Nagios --> Continuous montioring

Azure Devops
=============
Boards
Epic
issues bugs
features
user story
artifacts --> allows teams to share packages such as Maven, npm, NuGet, 
and more from public and private sources and integrate package sharing into your pipelines

Repos --> To connect with your repository
Pipelines --> To build pipelines

Az    vs    AWS

blobs        s3
Cosmos db    Dynamo db
Synapse      Redshift
SQL          DBS
ADF          Pipelines
ML studio    Sagemaker
PowerBi      Athena, glue






































